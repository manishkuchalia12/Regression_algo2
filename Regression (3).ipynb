{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ac6fa4-ad8c-4335-8c13-9c1450685922",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Ans:-Lasso Regression, or Least Absolute Shrinkage and Selection Operator, is a linear regression technique that adds a penalty term to the ordinary least squares (OLS) cost function. This penalty term is based on the absolute values of the coefficients, and it is designed to promote sparsity in the model, leading to feature selection. Lasso Regression is particularly useful when dealing with datasets with a large number of features, as it can drive some coefficients exactly to zero, effectively eliminating irrelevant or redundant predictors.\n",
    "\r\n",
    "Key Features of Lasso Regressio\n",
    "Sparsity and Feature Selection:\n",
    "\r\n",
    "Lasso encourages sparsity by driving some coefficients exactly to zero. This results in feature selection, as only a subset of the original features will have non-zero coefficients in the model.\r\n",
    "The sparsity induced by Lasso makes it well-suited for datasets with many features, where not all features contribute significantly to the prediction.\r\n",
    "Shrinkage of Coefficiets:\r\n",
    "\r\n",
    "Lasso Regression induces shrinkage on the coefficients, similar to Ridge Regression. However, the L1 penalty in Lasso (absolute values) leads to more aggressive shrinkage compared to the L2 penalty in Ridge Regression (squared values).\r\n",
    "Differences from Other Regression Techniques:\r\n",
    "L1 Regularization (AbsoluteValue):\r\n",
    "\r\n",
    "Lasso uses L1 regularization, which adds the absolute values of the coefficients to the cost function. This promotes sparsity by driving some coefficients exactly to zero.\r\n",
    "Ridge Regression, in contrast, uses L2 regularization, adding the squared values of the coefficients to the cost function.\r\n",
    "Variabl Selection:\r\n",
    "\r\n",
    "Lasso is unique in its ability to perform feature selection by setting some coefficients to exactly zero. Ridge Regression also shrinks coefficients but rarely eliminates them entirely.\r\n",
    "Ordinary Least Squares (OLS) regression does not include any regularization and estimates coefficients without shrinkage or feature selection.\r\n",
    "Handling Muticollinearity:\r\n",
    "\r\n",
    "Lasso can effectively handle multicollinearity (high correlation among features) by selecting a subset of relevant features and setting others to zero. It tends to choose one among a group of correlated features.n:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef073c-04b7-48b3-8076-464d34f1224f",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdfba00-ed55-4890-8946-bc5a6792091b",
   "metadata": {},
   "source": [
    "Ans:-The main advantage of using Lasso Regression in feature selection is its ability to automatically identify and select a subset of relevant features from a larger set. This is achieved through the sparsity-inducing property of the L1 regularization term, which leads to some coefficients being exactly set to zero. The primary benefits of Lasso Regression for feature selection include:\n",
    "\r\n",
    "Automatic Feature Selectio:\r\n",
    "\r\n",
    "Lasso Regression performs automatic feature selection by driving some coefficients to exactly zero. This means that certain features are effectively eliminated from the model.\r\n",
    "The model indicates which features are deemed most important for predicting the target variable.\r\n",
    "Simplicity and Interpretablity:\r\n",
    "\r\n",
    "The sparsity induced by Lasso results in a simpler and more interpretable model. The reduced set of features with non-zero coefficients provides a clear understanding of the predictors that contribute significantly to the response variable.\r\n",
    "The model becomes more transparent and easier to communicate.\r\n",
    "Handling High-Dimensinal Data:\r\n",
    "\r\n",
    "Lasso is particularly useful when dealing with high-dimensional datasets, where the number of features is much larger than the number of observations. In such scenarios, traditional linear regression techniques may overfit the data, but Lasso's feature selection helps prevent overfitting.\r\n",
    "Multicollinearit Mitigation:\r\n",
    "\r\n",
    "Lasso can effectively handle multicollinearity, which is the high correlation among predictor variables. By selecting a subset of relevant features, Lasso tends to choose one among a group of correlated features and sets others to zero.\r\n",
    "This is in contrast to some other regression techniques that may struggle with multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3524e5-5b2a-43d0-9a5b-4f647eb1dede",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Ans:-Interpreting the coefficients of a Lasso Regression model involves considering the magnitude and sign of the coefficients, as well as the sparsity-inducing property of L1 regularization. Here are the key points to consider when interpreting the coefficients of a Lasso Regression model:\n",
    "\r\n",
    "Magnitude of Coefficient:\r\n",
    "\r\n",
    "The magnitude of each coefficient indicates the strength of the relationship between the corresponding feature and the target variable.\r\n",
    "Larger magnitudes suggest a more significant impact on the target.\r\n",
    "Sign of Coefficents:\r\n",
    "\r\n",
    "The sign of each coefficient indicates the direction of the relationship between the feature and the target variable.\r\n",
    "Positive coefficients imply a positive relationship, while negative coefficients imply a negative relationship.\r\n",
    "Sparsity and Feature election:\r\n",
    "\r\n",
    "One of the distinctive features of Lasso Regression is its ability to drive some coefficients exactly to zero. This results in sparsity in the model, as only a subset of features has non-zero coefficients.\r\n",
    "Features with non-zero coefficients are deemed to be the most important predictors, while those with zero coefficients are effectively excluded from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c74213-d7ae-47a7-aa59-fc36fc02330b",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Ans:-Lasso Regression, by itself, is a linear regression technique and is not designed to handle non-linear regression problems. It assumes a linear relationship between the predictors and the target variable. However, it can be extended to address non-linear regression problems by incorporating non-linear transformations of the features or by combining it with non-linear modeling techniques.\n",
    "\r\n",
    "Here are two common approaches to handle non-linear regression problems using Lasso Regressio:\r\n",
    "\r\n",
    "Feature Transformaton:\r\n",
    "\r\n",
    "One straightforward way to handle non-linear relationships is to transform the input features using non-linear functions.\r\n",
    "You can introduce polynomial features by creating new features that are powers of the original fea \r\n",
    "3\r\n",
    " , and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5feae66-7b98-4d16-93d6-14eb85b3e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.normal(0, 1, size=100)\n",
    "\n",
    "# Create a Lasso Regression model with polynomial features\n",
    "degree = 2  # Degree of polynomial features\n",
    "lasso_model = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=0.1))\n",
    "lasso_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a584bd-7720-41c6-aa18-058f0851b586",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc60d7-551b-4e10-b8f6-2ca3f19a86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Ridge Regression:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit a Ridge Regression model\n",
    "alpha = 1.0  # Regularization parameter (adjust as needed)\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d46abd-aea2-49f8-b0e7-02e79c30be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2. Lasso Regression:\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit a Lasso Regression model\n",
    "alpha = 1.0  # Regularization parameter (adjust as needed)\n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99867b-e4b8-47b8-b028-5b811830836e",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Ans:-Yes, Lasso Regression can handle multicollinearity in the input features to some extent. Multicollinearity occurs when two or more features in a regression model are highly correlated, leading to instability in the coefficient estimates. Lasso Regression, by introducing an L1 regularization term, has the ability to address multicollinearity in a specific way:\r\n",
    "\r\n",
    "Feature Selection:\r\n",
    "\r\n",
    "Lasso Regression has a feature selection property that tends to drive some coefficients exactly to zero.\r\n",
    "When faced with multicollinearity, Lasso may select one feature from a group of highly correlated features and set the coefficients of the others to zero.\r\n",
    "Sparse Model:\r\n",
    "\r\n",
    "The sparsity-inducing nature of L1 regularization helps in creating a sparse model, where only a subset of features has non-zero coefficients.\r\n",
    "In the context of multicollinearity, this sparsity can result in a more stable model by effectively choosing a representative feature from the correlated group.\r\n",
    "Reduced Sensitivity to Small Changes:\r\n",
    "\r\n",
    "Lasso Regression can reduce the sensitivity of coefficient estimates to small changes in the input features, providing stability in the presence of multicollinearity.\r\n",
    "This is because, when multicollinearity is present, small changes in the data may lead to large changes in the estimated coefficients. By selecting a subset of features, Lasso mitigates this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe7139-8cc0-4d39-9e1f-a5d38eefb348",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f222a748-2cbf-45c1-8317-d6448cfe5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Cross-Validation:\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Use LassoCV for cross-validated Lasso Regression\n",
    "lasso_cv_model = LassoCV(alphas=[0.001, 0.01, 0.1, 1.0, 10.0], cv=5)\n",
    "lasso_cv_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Optimal alpha (lambda) chosen by cross-validation\n",
    "optimal_alpha = lasso_cv_model.alpha_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b93a0-280f-4b3f-8f25-ee4a72d59a80",
   "metadata": {},
   "outputs": [],
   "source": [
    " LassoCV:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
